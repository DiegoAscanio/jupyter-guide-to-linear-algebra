{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Diagonalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we describe a basic computational method that can be used to approximate eigenvalues.\n",
    "\n",
    "- Demonstrate SciPy Method\n",
    "- Explain basis for Power Method\n",
    "- Show examples\n",
    "- Power Method with Shifts?\n",
    "\n",
    "Since the action of an $n\\times n$ matrix $A$ on its eigenvectors is easy to understand, we might try to understand the action of a matrix on an arbitrary vector $X$ by writing $X$ as a linear combination of eigenvectors.  This will always be possible if the eigenvectors *form a basis for* $\\mathbb{R}^n$.  Suppose $\\{V_1, V_2, ..., V_n\\}$ are the eigenvectors of $A$ and they do form a basis for $\\mathbb{R}^n$.  Then for any vector $X$, we can write \n",
    "$X = c_1V_1 + c_2V_2 + ... c_nV_n$.  The product $AX$ can easily be computed then by multiplying each eigenvector component by the corresponding eigenvalue.\n",
    "\n",
    "$$\n",
    "\\begin{eqnarray*}\n",
    "AX & = & A(c_1V_1 + c_2V_2 + ... c_nV_n) \\\\\n",
    "   & = & c_1AV_1 + c_2AV_2 + ... c_nAV_n \\\\\n",
    "   & = & c_1\\lambda_1V_1 + c_2\\lambda_2V_2 + ... c_n\\lambda_nV_n\n",
    "\\end{eqnarray*}\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 1:  Shear\n",
    "\n",
    "Consider a matrix that represents a shear along the line $x_1=x_2$.  Demonstrate the action of the matrix as the sum of actions on the eigenvector components."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computation with SciPy\n",
    "\n",
    "We now demonstrate how to compute eigenvalues and eigenvectors for any square matrix using the function $\\texttt{eig}$ from the $\\texttt{linalg}$ module of the SciPy library.  This function accepts an $n\\times n$ array representing a matrix and returns two arrays, one containing the eigenvalues, the other the eigenvectors.  We examine the usage by supplying our projection matrix as the argument. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.2 -0.4]\n",
      " [-0.4  0.8]]\n",
      "\n",
      "\n",
      "[0.+0.j 1.+0.j]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import laguide as lag\n",
    "import scipy.linalg as SLA\n",
    "\n",
    "A = np.array([[0.2, -0.4],[-0.4, 0.8]])\n",
    "print(A)\n",
    "print('\\n')\n",
    "\n",
    "evalues,evectors = SLA.eig(A)\n",
    "\n",
    "print(evalues)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The array of eigenvalues contains two entries that are in the format of $\\alpha + \\beta j$, which represents a complex number.  The symbol $j$ is used for the imaginary unit.  The value of $\\beta$ is zero for both of the eigenvalues, which means that they are both real numbers.  The results confirm our conclusions that the eigenvalues are 0 and 1.\n",
    "\n",
    "Next let's look at the array of eigenvectors.  We can slice the array into columns to give us convenient access to the vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.89442719  0.4472136 ]\n",
      " [-0.4472136  -0.89442719]]\n"
     ]
    }
   ],
   "source": [
    "print(evectors)\n",
    "\n",
    "V_1 = evectors[:,0:1]\n",
    "V_2 = evectors[:,1:2]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We may not recognize the eigenvectors as those we found previously, but recall that eigenvectors are not unique.  The $\\texttt{eig}$ function scales all the eigenvectors to unit length, and we arrive at the same result if we scale our choice of eigenvector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-2]\n",
      " [-1]]\n",
      "\n",
      "\n",
      "[[-0.89442719]\n",
      " [-0.4472136 ]]\n"
     ]
    }
   ],
   "source": [
    "V = np.array([[-2],[-1]])\n",
    "print(V)\n",
    "print('\\n')\n",
    "print(V/lag.Magnitude(V))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example\n",
    "\n",
    "Let's try another example with a $3\\times 3$ matrix.\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "B = \\left[ \\begin{array}{rrr} 1 & 2 & 0  \\\\ 2 & -1 & 4 \\\\  0 & 3 & 1\\end{array}\\right]\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-4.12310563+0.j  1.        +0.j  4.12310563+0.j]\n",
      "\n",
      "\n",
      "[[ 3.19250163e-01  8.94427191e-01  4.19279023e-01]\n",
      " [-8.17776152e-01 -1.03817864e-16  6.54726338e-01]\n",
      " [ 4.78875244e-01 -4.47213595e-01  6.28918535e-01]]\n"
     ]
    }
   ],
   "source": [
    "B = np.array([[1,2, 0],[2,-1,4],[0,3,1]])\n",
    "evalues, evectors = SLA.eig(B)\n",
    "\n",
    "print(evalues)\n",
    "print('\\n')\n",
    "print(evectors)\n",
    "\n",
    "V_1 = evectors[:,0:1]\n",
    "V_2 = evectors[:,1:2]\n",
    "V_3 = evectors[:,2:3]\n",
    "\n",
    "E_1 = evalues[0]\n",
    "E_2 = evalues[1]\n",
    "E_3 = evalues[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We don't have the exact eigenvalues, but we can check that $BV_i - \\lambda_iV_i = 0$ for $i=1, 2, 3$, allowing of for small precision error.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-4.44089210e-16+0.j]\n",
      " [ 2.22044605e-15+0.j]\n",
      " [-1.11022302e-15+0.j]]\n"
     ]
    }
   ],
   "source": [
    "print(B@V_1-E_1*V_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of $BV_i - \\lambda_iV_i = 0$ for each $i$, we can package the calculations into a single matrix multiplication.  If $S$ is the matrix with columns $V_i$, then $BS$ is the matrix with columns $BV_i$.  This matrix should be compared to the matrix that has $\\lambda_iV_i$ as its columns.  To construct this matrix we use a diagonal matrix $D$, that has the $\\lambda_i$ as its diagonal entries.  The matrix product $SD$ will then have columns $\\lambda_iV_i$.\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "SD = \\left[ \\begin{array}{c|c|c} & & \\\\ V_1 & V_2 & V_3 \\\\ & & \\end{array} \\right]\n",
    "\\left[ \\begin{array}{rrr} \\lambda_1 & 0 & 0  \\\\ 0 & \\lambda_2 & 0 \\\\  0 & 0 & \\lambda_3 \\end{array}\\right]= \n",
    "\\left[ \\begin{array}{c|c|c} & & \\\\ \\lambda_1V_1 & \\lambda_2V_2 & \\lambda_3V_3 \\\\ & & \\end{array}\\right]\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "We can now simply check that $BS-SD = 0$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-4.12310563+0.j  0.        +0.j  0.        +0.j]\n",
      " [ 0.        +0.j  1.        +0.j  0.        +0.j]\n",
      " [ 0.        +0.j  0.        +0.j  4.12310563+0.j]]\n",
      "\n",
      "\n",
      "[[-4.44089210e-16+0.j -6.66133815e-16+0.j  4.44089210e-16+0.j]\n",
      " [ 2.22044605e-15+0.j  5.47907073e-16+0.j  0.00000000e+00+0.j]\n",
      " [-1.11022302e-15+0.j -1.11022302e-16+0.j  0.00000000e+00+0.j]]\n"
     ]
    }
   ],
   "source": [
    "S = evectors\n",
    "\n",
    "#  Since the eigenvalues are complex, it is best to use an array of complex numbers for D\n",
    "D = np.zeros((3,3),dtype='complex128')\n",
    "for i in range(3):\n",
    "    D[i,i] = evalues[i]\n",
    "\n",
    "print(D)\n",
    "print('\\n')\n",
    "print(B@S-S@D)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Diagonal Factorization\n",
    "\n",
    "The calculation that we used to verify the eigenvalues an eigenvectors is also very useful to construct another important matrix factorization.  Suppose that $A$ is an $n\\times n$ matrix, $D$ is a diagonal matrix with the eigenvalues of $A$ along its diagonal, and $S$ is the $n\\times n$ matrix with the eigenvectors of $A$ as its columns.  We have just seen that $AS=SD$.  If $S$ is invertible, we may also write $A=SDS^{-1}$, which is known as the **diagonalization** of $A$.\n",
    "\n",
    "The diagonalization of $A$ is important because it provides us with a complete description of the action of $A$ in terms of its eigenvectors.  Consider an arbitrary vector $X$, and the product $AX$, computed by using the three factors in the diagonization.\n",
    "\n",
    "- $S^{-1}X$ computes the coordinates of $X$ in terms of the eigenvectors of $A$.\n",
    "- Multiplication by $D$ then simply scales each coordinate by the corresponding eigenvalue.\n",
    "- Multiplication by $S$ gives the results with respect to the standard basis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
