{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approximating Eigenvalues"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we look at some methods that can be used to approximate the eigenvalues of a matrix $A$.  Although it is possible to find the exact eigenvalues for small matrices, the approach is impractical for larger matrices.\n",
    "\n",
    "Most introductory textbooks demonstrate a direct way to compute eigenvalues of an $n\\times n$ matrix $A$ by computing roots of an associated $n$th degree polynomial, known as the *characteristic polynomial*.  For example, suppose $A$ is a $2\\times 2$ matrix.\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "A = \\left[ \\begin{array}{rr} a & b  \\\\ c & d \\end{array}\\right]\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "The eigenvalues of $A$ are solutions to the quadratic equation $\\lambda^2 - (a+d)\\lambda + ad-bc = 0$, which can be written explicitly in terms of $a$, $b$, $c$, and $d$ using the quadratic formula.  The challenges with larger matrices are that the polynomial is more difficult to construct, and the roots cannot be easily found with a formula.\n",
    "\n",
    "The algorithms we describe in the section are iterative methods.  They generate a sequence of values $\\{\\lambda^{(1)}, \\lambda^{(2)}, \\lambda^{(3)}, ... \\}$ that approach a true eigenvalue of the matrix under consideration. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Power Method\n",
    "\n",
    "The first algorithm we introduce for approximating eigenvalues is known as the **Power Method**.  This method generates a sequence of vectors by repeated matrix multiplication.  Under suitable conditions, the sequence of vectors approaches the eigenvector associated with the eigenvalue that is largest in absolute value.    \n",
    "\n",
    "For the simplest explanation, suppose that $A$ is an $n\\times n$ diagonalizable matrix with eigenvectors $\\{V_1, V_2, ... V_n\\}$, and that $\\lambda_1$ is the eigenvalue of $A$ that is largest in absolute value.  To begin the Power Method, we choose any nonzero vector and label it $X^{(0)}$.  We can express $X^{(0)}$  as a linear combination of the eigenvectors since they form a basis for $\\mathbb{R}^n$.\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "X^{(0)} = c_1V_1 + c_2V_2 + ... c_nV_n\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "We now form a sequence of vectors $X^{(1)}$, $X^{(2)}$, $X^{(3)}$, ..., by setting $X^{(m)}= AX^{(m-1)}$.  Each of these vectors is also easly expressed in terms of the eigenvectors.\n",
    "\n",
    "$$\n",
    "\\begin{eqnarray*}\n",
    "X^{(1)} = AX^{(0)} & = & c_1AV_1 + c_2AV_2 + ... c_nAV_n \\\\\n",
    "                   & = & c_1\\lambda_1V_1 + c_2\\lambda_2V_2 + ... c_n\\lambda_nV_n \\\\\n",
    "X^{(2)} = AX^{(1)} & = & c_1\\lambda_1AV_1 + c_2\\lambda_2AV_2 + ... c_n\\lambda_nAV_n \\\\\n",
    "                   & = & c_1\\lambda_1^2V_1 + c_2\\lambda_2^2V_2 + ... c_n\\lambda_n^2V_n \\\\\n",
    "                   & \\vdots & \\\\\n",
    "X^{(m)} = AX^{(m-1)} & = & c_1\\lambda_1^{m-1}AV_1 + c_2\\lambda_2^{m-1}AV_2 + ... c_n\\lambda_n^{m-1}AV_n \\\\\n",
    "                   & = & c_1\\lambda_1^mV_1 + c_2\\lambda_2^mV_2 + ... c_n\\lambda_n^mV_n \n",
    "\\end{eqnarray*}\n",
    "$$\n",
    "\n",
    "In the expression for $X^{(m)}$, we can then factor out $\\lambda_1^m$ to understand what happens as $m$ gets large.\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "X^{(m)} =  \\lambda_1^m\\left(c_1V_1 + c_2\\left(\\frac{\\lambda_2}{\\lambda_1}\\right)^mV_2 + ... c_n\\left(\\frac{\\lambda_n}{\\lambda_1}\\right)^mV_n\\right) \n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "If $|\\lambda_1| > |\\lambda_i|$ for all $i\\neq 1$, then $|\\lambda_i/\\lambda_1|< 1$ and $(\\lambda_i/\\lambda_1)^m$ will approach zero as $m$ gets large.  This means that if we repeatedly multiply a vector by the matrix $A$, eventually we will get a vector that is very nearly in the direction of the eigenvector that corresponds to the $\\lambda_1$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's demonstrate the calculation on the matrix shown here before we discuss the method further.\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "A = \\left[ \\begin{array}{rrrr} -2 & 6 & 2 & -8 \\\\ -6 & 0 & 12 & 12 \\\\ -6 & 0 & 12 & 12 \\\\ -10 & 3 & 7 & 14 \\end{array}\\right]\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "As a matter of practicality, it is common to scale the vectors in the sequence to unit length as the Power Method is applied.  If the vectors in the squence are not scaled, their magnitudes will grow of $\\lambda_1>1$ or decay if $\\lambda_1<1$.    Since all components of the vectors get divided by the same factor when the vector is scaled, this step doesn't change the ultimate behavior of the sequence.  The scaled sequence of vectors still approaches the direction of the eigenvector. \n",
    "\n",
    "We choose an arbitrary $X^{(0)}$ and calculate $X^{(20)}$ using the following rule.\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "X^{(m)}=\\frac{AX^{(m-1)}}{||AX^{(m-1)}||}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.57523994e-12]\n",
      " [-5.77350269e-01]\n",
      " [-5.77350269e-01]\n",
      " [-5.77350269e-01]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import laguide as lag\n",
    "A = np.array([[-2, 6, 2, -8],[-6, 0, 12, 12],[-6, 0, 12, 12],[-10, 3, 7, 14]])\n",
    "X = np.array([[1],[0],[0],[0]])\n",
    "\n",
    "m = 0\n",
    "while (m < 20):\n",
    "    X = A@X\n",
    "    X = X/lag.Magnitude(X)\n",
    "    m = m + 1\n",
    "    \n",
    "print(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now if $X$ is the eigenvector of $A$ with unit magnitude, then $|AX| = |\\lambda_1X| = |\\lambda_1|$.  We can therefor approximate $|\\lambda_1|$ with $|AX|$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24.000000000020005\n"
     ]
    }
   ],
   "source": [
    "print(lag.Magnitude(A@X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It appears that 24 is an estimate for $\\lambda_1$.   To determine if our calculation is correct, we can compare $AX$ with $\\lambda_1X$.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-4.09570156e-11]\n",
      " [-9.45021839e-12]\n",
      " [-9.45021839e-12]\n",
      " [-1.57545088e-11]]\n"
     ]
    }
   ],
   "source": [
    "print(A@X - 24*X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indeed the difference $AX-24X$ small.  Note that in this case, we can even do the calculation with integer multiplication.  Notice that $X$ has 0 in the first entry and the other entries are equal.  If we set these entries to 1, the result is easy to calculate even without the aid of the computer.  (*Remember that we can change the magnitude of an eigenvector and it is still an eigenvector.*) \n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "AX = \\left[ \\begin{array}{rrrr} -2 & 6 & 2 & -8 \\\\ -6 & 0 & 12 & 12 \\\\ -6 & 0 & 12 & 12 \\\\ -10 & 3 & 7 & 14 \\end{array}\\right]\n",
    "\\left[ \\begin{array}{r} 0 \\\\ 1\\\\ 1 \\\\ 1 \\end{array}\\right] =\n",
    "\\left[ \\begin{array}{r} 0 \\\\ 24\\\\ 24 \\\\ 24 \\end{array}\\right] = 24X\n",
    "\\end{equation}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In practice, we do not know how many iterations we need to perform in order to get a good approximation of the eigenvector eigenvalue.  Instead we should specify a condition upon which we will be satisfied with the approximation and terminate the iteration.  For example, since $||AX^{(m)}||\\approx \\lambda_1$ and $AX^{(m)}\\approx \\lambda_1X^{(m)}$ we might require that $AX^{(m)} - ||AX^{(m)}||X^{(m)} < \\epsilon$ for some small number $\\epsilon$ known as a tolerance.  This condition ensures that $X^{(m)}$ functions roughly like an eigenvector. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eigenvector is approximately:\n",
      "[[ 1.65181395e-06]\n",
      " [-5.77350269e-01]\n",
      " [-5.77350269e-01]\n",
      " [-5.77350269e-01]]\n",
      "Eigenvalue is approximately:\n",
      "24.00002098082306\n",
      "Magnitude of difference is:\n",
      "4.3284704409694366e-05\n"
     ]
    }
   ],
   "source": [
    "X = np.array([[1],[0],[0],[0]])\n",
    "\n",
    "m = 0\n",
    "tolerance = 0.0001\n",
    "MAX_ITERATIONS = 100\n",
    "\n",
    "## Compute difference in stopping condition\n",
    "## Assign Y = AX to avoid computing AX multiple times\n",
    "Y = A@X\n",
    "difference = Y - lag.Magnitude(Y)*X\n",
    "\n",
    "while (m < MAX_ITERATIONS and lag.Magnitude(difference) > tolerance):\n",
    "    X = Y\n",
    "    X = X/lag.Magnitude(X)\n",
    "\n",
    "    ## Compute difference in stopping condition\n",
    "    Y = A@X\n",
    "    difference = Y - lag.Magnitude(Y)*X\n",
    "    \n",
    "    m = m + 1\n",
    "    \n",
    "print(\"Eigenvector is approximately:\")\n",
    "print(X)\n",
    "print(\"Eigenvalue is approximately:\")\n",
    "print(lag.Magnitude(Y))\n",
    "print(\"Magnitude of difference is:\")\n",
    "print(lag.Magnitude(difference))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A more common condition to require is that $||X^{(m)} - X^{(m-1})|| < \\epsilon$ for a given tolerance $\\epsilon$.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eigenvector is approximately:\n",
      "[[ 2.64294012e-05]\n",
      " [-5.77350269e-01]\n",
      " [-5.77350269e-01]\n",
      " [-5.77350269e-01]]\n",
      "Eigenvalue is approximately:\n",
      "24.000335690312877\n",
      "Magnitude of difference is:\n",
      "8.434774776937697e-05\n"
     ]
    }
   ],
   "source": [
    "X = np.array([[1],[0],[0],[0]])\n",
    "\n",
    "m = 0\n",
    "tolerance = 0.0001\n",
    "MAX_ITERATIONS = 100\n",
    "\n",
    "difference = X\n",
    "\n",
    "while (m < MAX_ITERATIONS and lag.Magnitude(difference) > tolerance):\n",
    "    X_previous = X\n",
    "    X = A@X\n",
    "    X = X/lag.Magnitude(X)\n",
    "\n",
    "    ## Compute difference in stopping condition\n",
    "    difference = X - X_previous\n",
    "    \n",
    "    m = m + 1\n",
    "    \n",
    "print(\"Eigenvector is approximately:\")\n",
    "print(X)\n",
    "print(\"Eigenvalue is approximately:\")\n",
    "print(lag.Magnitude(A@X))\n",
    "print(\"Magnitude of difference is:\")\n",
    "print(lag.Magnitude(difference))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Issues with Power Method:\n",
    "\n",
    "- Most obviously, it only applies to the largest eigenvalue.  This is not a huge detriment since applications often only require an approximation of the largest eigenvalue.  Also, as we will demonstrate, it is possible to modify the method to approximate the other eigenvalues.\n",
    "\n",
    "- The power method is slow to converge.  Without getting into what \"slow\" means, we can see that if $|\\lambda_1|$ is close to $|\\lambda_2|$, then $|\\lambda_1/\\lambda_2|^m$ approaches zero more slowly as $m$ gets large.\n",
    "\n",
    "- The method may perform poorly if the $V_1$ component of $X^{(0)}$ is small.\n",
    "\n",
    "- What happens with complex eigenvalues?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2.40000000e+01+0.j -6.00000000e+00+0.j  6.00000000e+00+0.j\n",
      " -3.99305433e-15+0.j]\n",
      "[ 4.94568128e-16 -5.77350269e-01 -5.77350269e-01 -5.77350269e-01]\n"
     ]
    }
   ],
   "source": [
    "import scipy.linalg as SLA\n",
    "\n",
    "evalues, evectors = SLA.eig(A)\n",
    "print(evalues)\n",
    "print(evectors[:,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inverse Power Method\n",
    "\n",
    "The **Inverse Power Method** is a modified version of the Power Method that allows us to approximate eigenvalues that are *not the largest*.  All that is needed to make the modification is two simple facts that relate changes in a matrix to changes in the eigenvalues of that matrix.  Let's suppose that $A$ is an invertible $n\\times n$ matrix with eigenvalue $\\lambda$ and correpsonding eigenvector $V$, so that $AV=\\lambda V$.  If we multiply this equation by $A^{-1}$, we get $V=\\lambda A^{-1}V$, which can then be divided by $\\lambda$ to illustrate the useful fact.\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "A^{-1}V = \\frac{1}{\\lambda}V\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "If $\\lambda$ is an eigenvalue of $A$, then $\\lambda^{-1}$ is an eigenvalue of $A$.  Furthermore the eigenvector of $A$ is also an eigenvector of $A^{-1}$.  The important point here is that if $\\lambda_n$ is the smallest eigenvalue of $A$, then $\\lambda_n^{-1}$ is the *largest* eigenvector of $A^{-1}$.  If we want to approximate the smallest eigenvalue of $A$, we can just apply the Power Method to $A^{-1}$.\n",
    "\n",
    "We demonstrate the calculation for the following $3\\times 3$ matrix.\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "A = \\left[ \\begin{array}{rrrr} 9 & -1 & -3 \\\\ 0 & 6 & 0 \\\\ -6 & 3 & 6 \\end{array}\\right]\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "Again we choose an aribitrary $X^{(0)}$, and generate a sequence of vectors by mulitplying by $A^{-1}$ and scaling the result to unit length.\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "X^{(m)}=\\frac{A^{-1}X^{(m-1)}}{||A^{-1}X^{(m-1)}||}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eigenvector is approximately:\n",
      "[[0.44722997]\n",
      " [0.        ]\n",
      " [0.894419  ]]\n",
      "Eigenvalue is A inverse is approximately:\n",
      "0.333334859169165\n",
      "Eigenvalue is A is approximately:\n",
      "2.9999862675403755\n"
     ]
    }
   ],
   "source": [
    "X = np.array([[1],[0],[0]])\n",
    "\n",
    "m = 0\n",
    "tolerance = 0.0001\n",
    "MAX_ITERATIONS = 100\n",
    "\n",
    "difference = X\n",
    "A = np.array([[9,-1,-3],[0,6,0],[-6,3,6]])\n",
    "A_inv = lag.Inverse(A)\n",
    "\n",
    "while (m < MAX_ITERATIONS and lag.Magnitude(difference) > tolerance):\n",
    "    X_previous = X\n",
    "    X = A_inv@X\n",
    "    X = X/lag.Magnitude(X)\n",
    "\n",
    "    ## Compute difference in stopping condition\n",
    "    difference = X - X_previous\n",
    "    \n",
    "    m = m + 1\n",
    "    \n",
    "print(\"Eigenvector is approximately:\")\n",
    "print(X)\n",
    "print(\"Eigenvalue is A inverse is approximately:\")\n",
    "print(lag.Magnitude(A_inv@X))\n",
    "print(\"Eigenvalue is A is approximately:\")\n",
    "print(1.0/lag.Magnitude(A_inv@X))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The exact value of the smallest eigenvalue of $A$ is 3, which again can be verified by calculation.\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "AV = \\left[ \\begin{array}{rrrr} 9 & -1 & -3 \\\\ 0 & 6 & 0 \\\\ -6 & 3 & 6 \\end{array}\\right]\n",
    "\\left[ \\begin{array}{r} 1 \\\\ 0\\\\ 2 \\end{array}\\right] =\n",
    "\\left[ \\begin{array}{r} 3 \\\\ 0 \\\\ 6 \\end{array}\\right] = 3V\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "In our discussion of [Inverse Matrices](Inverse_Matrices.ipynb) we noted that the construction of an inverse matrix is quite expensive since it requires the solution of $n$ systems of size $n\\times n$.  An alternative to constructing $A^{-1}$ and computing the product $A^{-1}X^{(m-1)}$ is to solve the system $AY=X^{(m-1)}$ and then scale $Y$ to unit length to obtain $X^{(m)}$.  This means that we solve one $n\\times n$ system for every iteration.  This appears to require more work than the construction of $A^{-1}$, but in fact it is less since every system involves the same coefficient matrix.  We can therefore save much work by performing elimination only once and storing the result in an $LU$ factorization.  With the the matrix $A$ factored, each system $AY=X^{(m-1)}$ only requires one forward substitution and one backward substitution.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12.+0.j  3.+0.j  6.+0.j]\n",
      "[ 0.70710678  0.         -0.70710678]\n"
     ]
    }
   ],
   "source": [
    "import scipy.linalg as SLA\n",
    "\n",
    "B = np.array([[9,-1,-3],[0,6,0],[-6,3,6]])\n",
    "evalues, evectors = SLA.eig(B)\n",
    "print(evalues)\n",
    "print(evectors[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References\n",
    "\n",
    "- Burden, Richard L. et al. *Numerical Analysis*. 10th ed., Cengage Learning, 2014\n",
    "- Golub, Gene H. and Charles F. Van Loan. *Matrix Computations*., The Johns Hopkins University Press, 1989\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
